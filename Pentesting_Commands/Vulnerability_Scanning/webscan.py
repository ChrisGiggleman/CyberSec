import requests
import argparse
import json
from bs4 import BeautifulSoup

# ------------------------------
# Vulnerability Check Modules
# ------------------------------
def check_headers(url):
    """Check for missing security headers."""
    findings = []
    try:
        r = requests.get(url, timeout=5)
        headers = r.headers
        required_headers = [
            "X-Frame-Options",
            "X-Content-Type-Options",
            "Strict-Transport-Security",
            "Content-Security-Policy"
        ]
        for h in required_headers:
            if h not in headers:
                findings.append(f"Missing header: {h}")
    except Exception as e:
        findings.append(f"Header check error: {e}")
    return findings


def check_sql_injection(url):
    """Very basic SQLi test."""
    findings = []
    payloads = ["' OR '1'='1", "'--", "' OR 'a'='a"]
    for p in payloads:
        try:
            r = requests.get(url, params={"id": p}, timeout=5)
            if "sql" in r.text.lower() or "mysql" in r.text.lower():
                findings.append(f"Possible SQL Injection with payload: {p}")
        except Exception as e:
            findings.append(f"SQLi check error: {e}")
    return findings


def check_xss(url):
    """Basic reflected XSS test."""
    findings = []
    payload = "<script>alert('xss')</script>"
    try:
        r = requests.get(url, params={"q": payload}, timeout=5)
        if payload in r.text:
            findings.append("Possible XSS vulnerability (reflected)")
    except Exception as e:
        findings.append(f"XSS check error: {e}")
    return findings


# ------------------------------
# Crawler
# ------------------------------
def crawl_site(url, depth=1):
    visited = set()
    to_visit = [url]
    found_urls = []

    for _ in range(depth):
        new_urls = []
        for link in to_visit:
            if link in visited:
                continue
            visited.add(link)
            try:
                r = requests.get(link, timeout=5)
                soup = BeautifulSoup(r.text, "html.parser")
                for a in soup.find_all("a", href=True):
                    href = a['href']
                    if href.startswith("http"):
                        new_urls.append(href)
                        found_urls.append(href)
            except:
                pass
        to_visit = new_urls
    return found_urls


# ------------------------------
# Main
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="Custom Web Vulnerability Scanner")
    parser.add_argument("-u", "--url", help="Target URL")
    parser.add_argument("-l", "--list", help="File with list of targets")
    parser.add_argument("--crawl", type=int, default=0, help="Crawl depth")
    parser.add_argument("--checks", help="Comma-separated checks: sql,xss,headers")
    parser.add_argument("-o", "--output", help="Save results to JSON file")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose mode")
    args = parser.parse_args()

    targets = []
    if args.url:
        targets.append(args.url)
    if args.list:
        with open(args.list, "r") as f:
            targets.extend([line.strip() for line in f if line.strip()])

    checks = []
    if args.checks:
        checks = args.checks.split(",")

    results = {}

    for target in targets:
        if args.verbose:
            print(f"[*] Scanning {target}...")

        results[target] = {"findings": []}

        # Crawl
        if args.crawl > 0:
            crawled = crawl_site(target, args.crawl)
            results[target]["crawled_urls"] = crawled
            if args.verbose:
                print(f"[+] Found {len(crawled)} URLs during crawl")

        # Run checks
        if "headers" in checks:
            results[target]["findings"].extend(check_headers(target))
        if "sql" in checks:
            results[target]["findings"].extend(check_sql_injection(target))
        if "xss" in checks:
            results[target]["findings"].extend(check_xss(target))

    # Save or print results
    if args.output:
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2)
        print(f"[+] Results saved to {args.output}")
    else:
        print(json.dumps(results, indent=2))


if __name__ == "__main__":
    main()
